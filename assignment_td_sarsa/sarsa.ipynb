{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA: State-Action-Reward-State-Action\n",
    "\n",
    "En este ejercicio vamos a implementar el algoritmo de SARSA como un ejemplo de los métodos on-policy para el aprendizaje por refuerzo. Esto es, crear agentes que aprenden a alcanzar un objetivo específico.\n",
    "\n",
    "El método de SARSA se basa en el cálculo de los q-valores utilizando los valores calculados para el estado de llegada siguiendo la fórmula de actualización de los q-valores:\n",
    "\n",
    "$Q(s,a) \\leftarrow (1-\\alpha)Q(s,a) + \\alpha[R(s) + \\gamma Q(s',a')] $\n",
    "\n",
    "Para implementar SARSA definiremos un agente, `sarsa_agent.py` el cual utilizaremos para interactuar con el ambiente de Gridworld.\n",
    "\n",
    "#### Task 1\n",
    "1.\tImplemente la classe `SARSA` con cinco atributos:\n",
    "    - `epsilon` que corresponde a la estrategia de aprendizaje $\\epsilon$-greedy, `0.9` por defecto.\n",
    "    - `gamma` que corresponde al factor de decuento a utilizar, `0.96` por defecto.\n",
    "    - `alpha` que corresponde a la taza de aprendizajem `0.81` por defecto.\n",
    "    - `Q` que almacena los q-valores del agente.\n",
    "    - `env` que es una referencia al ambiente.\n",
    "\n",
    "2. El comportamiento del agente (la interacción con el ambiente) esta dado por los métodos:\n",
    "    - `choose_action` que recibe un estado como parámetro y retorna la acción a ejecutar para dicho estado siguiendo una estrategia $\\epsilon$-greedy.\n",
    "    - `action_function` que recibe como parámetro los componentes de SARSA (estado1, acción1, recompensa, estado2, acción2) y calcula el q-valor `Q(estdo1, acción1)`.\n",
    "\n",
    "3. La interacción entre el agente y el ambiente inicia desde el ambiente, que ejecuta cada interacción de SARSA para cada episodio.\n",
    "(1) La interacción comienza decidiendo la acción a tomar para el estado actual (la cual esta dada por el agente), (2) luego debemos ejecutar la acción, obteniendo el estado de llegada y la recompensa de ejecutar dicha acción, (3) luego calculamos la acción a tomar para el estado de llegada, (4) por último calculamos el q-valor definido por la función de las acciones.\n",
    "\n",
    "#### Task 2\n",
    "Implemente el ambiente de cliff-walk (basado en el ambiente de Gridworld utilizdo anteriormente) y resulevalo utilizando el método de SARSA.\n",
    "Recuerde que en este ambiente la recompensa por caer al barranco es de -100 y la recompensa de cada paso es -1.\n",
    "Para la ejecución vamos a suponer acciones determinísticas.\n",
    "\n",
    "![cliff-walk](cliff-walk.png)\n",
    "\n",
    "Además responda las siguientes preguntas\n",
    "1. ¿Cuál es el comportamiento del agente si utilizamos un factor de descuento de 1?\n",
    "2. ¿Cómo podemos minimizar la trayectoria del agente entre el estado inicial y el estado de llegada?\n",
    "\n",
    "Justifique sus respuestas con ejecuciones reales del agente."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:23:00.089484Z",
     "start_time": "2024-10-02T23:22:59.886480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from assignment_td_sarsa.environment_world import EnvironmentWorld\n",
    "from assignment_td_sarsa.sarsa_agent import SarsaAgent\n",
    "\n",
    "cliff_world = EnvironmentWorld([\n",
    "    ['-1'] * 12,\n",
    "    ['-1'] * 12,\n",
    "    ['-1'] * 12,\n",
    "    ['S'] + ['-100'] * 10 + ['1000']\n",
    "],\n",
    "    terminal_states=[(x, 3) for x in range(1, 12)], noise=0.0)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:23:02.380898Z",
     "start_time": "2024-10-02T23:23:02.374895Z"
    }
   },
   "cell_type": "code",
   "source": "cliff_world",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     10    11\n",
       "0  -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "1  -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "2  -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "3  SC  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:23:04.660564Z",
     "start_time": "2024-10-02T23:23:04.658355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sarsa_agent = SarsaAgent(\n",
    "    world=cliff_world,\n",
    "    learning_rate=0.81,\n",
    "    discount_factor=0.96,\n",
    "    epsilon=0.9\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:25:00.069592Z",
     "start_time": "2024-10-02T23:23:08.429171Z"
    }
   },
   "cell_type": "code",
   "source": "sarsa_agent.iterate_learning(num_steps=10000000)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [01:51<00:00, 89581.86it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:25:14.084592Z",
     "start_time": "2024-10-02T23:25:14.080441Z"
    }
   },
   "cell_type": "code",
   "source": "sarsa_agent.print_policy()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0      1      2      3      4      5      6      7      8      9   \\\n",
      "0  right  right  right  right  right  right  right  right  right  right   \n",
      "1  right  right  right  right  right  right  right  right  right  right   \n",
      "2  right  right  right  right  right  right  right  right  right  right   \n",
      "3     up   None   None   None   None   None   None   None   None   None   \n",
      "\n",
      "      10    11  \n",
      "0  right  down  \n",
      "1  right  down  \n",
      "2  right  down  \n",
      "3   None  None  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T23:10:40.067771Z",
     "start_time": "2024-09-28T23:10:40.064303Z"
    }
   },
   "cell_type": "code",
   "source": "sarsa_agent.Q[(0,3)]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {<Action.DOWN: 'down'>: -79.90401239734301,\n",
       "             <Action.LEFT: 'left'>: -76.48526317653273,\n",
       "             <Action.RIGHT: 'right'>: -100.0,\n",
       "             <Action.UP: 'up'>: -81.39142602940687})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
