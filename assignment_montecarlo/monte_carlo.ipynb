{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de Monte Carlo\n",
    "\n",
    "En este ejercicio vamos a implementar la primera solución para los problemas de aprendizaje por refuerzo, los métodos de Monte Carlo. \n",
    "\n",
    "Recuerde que el método de Monte Carlo consiste en la colección de muestras calculando los valores para la secuencia completa de los estados hasta el estado final. Una vez se han coleccionado \"suficientes\" muestras, el valor de los estados se toma como el valor promedio de las muestras sobre las cuales apareció el estado.\n",
    "\n",
    "Para resolver problemas de aprendizaje por refuerzo utilizando el método de Monte Carlo crearemos un archivo `mcm.py`. Inicialmente utilizaremos este archivo para solucionar el problema sobre el ambiente de Gridworld (suponiendo un ruido de `0.25` para las acciones, es decir que la probabilidad de ejecutar la acción deseada es de 0.75 y una acción aleatoria (dividida en partes iguales) con probabilidad de 0.25).\n",
    "\n",
    "**Task 1**\n",
    "1.\tImplemente la classe `MCM` para solucionar Gridworld sin conocer los detalles del modelo de MDP para el problema. Es decir, en este caso, nuestro agente de `MCM` no tendrá acceso al `mdp` como era el caso para la iteración de valores o iteración de políticas.\n",
    "\n",
    "2. El comportamiento del agente (de Monte Carlo) esta dado por dos momentos. El proceso de recolección de muestras y el proceso de explotación de las mismas, es decir, el cálculo de la política del agente. Usted debe implementar el comportamiento del agente dado que, ejecutando episodios como muestras, sea capaz de calcular los valores para los estados.\n",
    "\n",
    "Para la implementación de `MCM` responda las siguientes preguntas. Tenga en cuenta que debe ejecutar su agente múltiples veces para poder observar el comportamiento (una sola instancia no nos puede llevar a ninguna conclución).\n",
    "Justifique su respuestas con análisis de la ejejcución y gráficas del comportamiento.\n",
    "1. ¿Cuantas muestras debe tomar el agente? Su implementación no debe utilizar este número como un parámetro o tenerlo como un factor predeterminado del agente.\n",
    "2. ¿Cómo se comparan los valores de `MCM` con respecto a los valores obtenidos en el ejercicio de iteración de valores `value_iteration`? ¿Porqué se da la diferencia si existe alguna, o porqué no existe ninguna diferencia?   \n",
    "3. ¿Cómo se compara la política obtenida utilizando `MCM` con respecto a la política obtenida en el jercicio de iteración de políticas `policy_iteration`? ¿Porqué se da la diferencia si existe alguna, o porqué no existe ninguna diferencia?\n",
    "4. ¿Cuál es el efecto de del factor de descuento sobre el método de Monte Carlo, calcule la solución de Gridworld con diferentes valores?\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T16:44:06.939933Z",
     "start_time": "2024-09-20T16:44:06.732363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mcm import MonteCarloAgent\n",
    "from environment_world import EnvironmentWorld\n",
    "\n",
    "grid_world = EnvironmentWorld([\n",
    "    ['S'] + [' '] * 9,\n",
    "    [' '] * 10,\n",
    "    [' ', '#', '#', '#', '#', ' ', '#', '#', '#', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', '-1', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', '+1', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', '-1', '-1', ' ', ' ', ' '],\n",
    "    [' '] * 10,\n",
    "    [' '] * 10\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T16:50:07.146776Z",
     "start_time": "2024-09-20T16:50:07.143128Z"
    }
   },
   "cell_type": "code",
   "source": "montecarlo_agent = MonteCarloAgent(grid_world, discount_factor=1, initial_epsilon=0.9, epsilon_decay=0.999)",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-20T16:50:40.042904Z"
    }
   },
   "cell_type": "code",
   "source": "montecarlo_agent.learn(max_episodes=10000, convergence_check_frequency=100, convergence_patience=10)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   5%|▌         | 541/10000 [00:25<12:46, 12.33it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T16:50:19.306479Z",
     "start_time": "2024-09-20T16:50:19.296239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy = [[None for i in range(grid_world.num_rows)] for j in range(grid_world.num_cols)] \n",
    "for x in range(grid_world.num_rows):\n",
    "    for y in range(grid_world.num_cols):\n",
    "        for action in grid_world.ACTIONS:\n",
    "            if montecarlo_agent.policy[(x, y)][action]:\n",
    "                policy[y][x] = action.value\n",
    "                break\n",
    "\n",
    "print(pd.DataFrame(policy))\n",
    "\n",
    "montecarlo_agent.policy[(4,5)]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1     2      3      4     5      6      7      8     9\n",
      "0  right   down  left     up     up    up     up  right     up    up\n",
      "1     up  right    up  right  right    up  right     up     up    up\n",
      "2     up   None  None   None   None    up   None   None   None  down\n",
      "3     up     up    up     up   None    up  right  right  right  down\n",
      "4  right     up    up     up   None  None     up   down   down  left\n",
      "5   down     up    up   down   None  None   left   left   left    up\n",
      "6  right     up  left   down   None    up  right     up   left  left\n",
      "7     up   left    up     up   None  None   None     up     up  left\n",
      "8     up   left    up  right  right  left   left     up     up    up\n",
      "9     up     up  left     up     up    up     up     up   left    up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {<Action.UP: 'up'>: 0.0,\n",
       "             <Action.RIGHT: 'right'>: 0.0,\n",
       "             <Action.DOWN: 'down'>: 0.0,\n",
       "             <Action.LEFT: 'left'>: 0.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
