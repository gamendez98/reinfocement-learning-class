{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0a6b89-5f5b-4706-8da8-c8d00ff69870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gridworld y su solución como MDPs\n",
    "\n",
    "En este trabajo definiremos el ambiente de Gridworld y su solución como un MDP.\n",
    "Gridworld es un ambiente clásico de prueba dentro del aprendizaje por refuerzo. Durante este taller definiremos el modelo básico del ambiente, que extenderemos incrementalmente de acuerdo a las necesidades del algoritmo de solución.\n",
    "\n",
    "## Ambiente 🌎\n",
    "\n",
    "El ambiente de ridworld se define como una cuadricula de `nxm`. El ambiente tiene obstaculos, es decir casillas por las cuales no puede pasar el agente. Al chocar con un obstaculo, el agente se mantiene terminaría en el mismo estado inicial. Además, el ambiente tiene una casilla de inicio, y algunas casillas de salida. Un ejemplo del ambiente para el caso `3x4` se muestra a continuación.\n",
    "\n",
    "![gridworld.png](https://raw.githubusercontent.com/FLAGlab/isis4222-rl/a502e264157729fcb8cc00d484e4a8e8e4734a15/week3/img/gridworld.png)\n",
    "\n",
    "En este ejemplo del ambiente el agente comienza en la casilla inferior izquierda y tiene como objetivo llegar a la casilla de salida verde, con recompensa 1. La otra casilla de salida, tiene recompensa -1.\n",
    "\n",
    "\n",
    "### Task 1.\n",
    "#### ¿Cómo podemos codificar el ambiente?\n",
    "\n",
    "De una definición completa del ambiente, como una clase de python llamada `Environment`, estableciendo:\n",
    "1. Un atributo que define la cuadrícula (`board`). El ambiente recibirá una matriz como parámetro describiendo la cuadrícula en el momento de su creación. Definiremos las casillas por las que puede pasar el agente como casillas vacias, las casillas por las que no puede pasar el agente con un valor none `None` y las casillas de salida con el valor asociado a la recompensa definidas para cada una de ellas.\n",
    "2. Un atributo `nrows` para almacenar la cantidad de filas de la cuadrícula.\n",
    "3. Un atributo `ncols` para almacenar la cantidad de columnas de la cuadrícula.\n",
    "4. Un atributo `initial_state` para almacenar el estado inicial del agente dentro del ambiente.\n",
    "5. Un atributo con el estado actual (`current_state`) en el que se encuentra el agente. El valor de `current_state` se definirá como una tupla \n",
    "\n",
    "Un ejemplo de la definición del tablero para el caso de 5x5 de la figura anterior se da a continuación.\n",
    "```\n",
    "board = [['', ' ', ' ',  '+1'],\n",
    "         [' ', '#', ' ',  '-1'],\n",
    "         ['S', ' ', ' ', ' ']]\n",
    "```\n",
    "En el ejemplo `S` denota el estado inicial y `'#'` la casilla prohibida (manejaremos esta convención para todos los ambientes de gridworld).\n",
    "\n",
    "#### Comportamiento del ambeinte\n",
    "\n",
    "Una vez definido el ambiente definimos su comportamiento. Para ello requerimos los siguientes métodos:\n",
    "1. `get_current_state` que no recibe parámetros y retorna el estado actual (la casilla donde se encuentra el agente)\n",
    "2. `get_posible_actions` que recibe el estado actual del agente como parámetro y retorna las acciones disponibles para dicho estado. Las acciones estarán dadas por su nombre (`'up', 'down', 'left', 'right'`). Como convención definiremos que el agente siempre puede moverse en todas las direcciones, donde un movimiento en dirección de un obstáculo o los límites del ambiente no tienen ningún efecto visible en la posición del agente.\n",
    "3. `do_action` que recibe como parámetro la acción a ejecutar y retorna el valor de la recompensa y el nuevo estado del agente, como un pareja `reward, new_state`\n",
    "4. `reset` que no recibe parámetros y restablece el ambiente a su estado inicial.\n",
    "5. `is_terminal` que no recibe parámetros y determina si el agente está en el estado final o no. En nuestro caso, el estado final estará determinado por las casillas de salida (i.e., con un valor definido).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855dbb21ff626c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.697151Z",
     "start_time": "2024-08-25T18:25:57.342651Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    UP = 'up'\n",
    "    RIGHT = 'right'\n",
    "    DOWN = 'down'\n",
    "    LEFT = 'left'\n",
    "\n",
    "\n",
    "State = Tuple[int, int]\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, board: List[List[str]]):\n",
    "        self.board = pd.DataFrame(board)\n",
    "        self.num_rows = len(board)\n",
    "        self.num_cols = len(board[0])\n",
    "        self.initial_state = (0, 0)\n",
    "        for i, row in self.board.iterrows():\n",
    "            for j, cell in enumerate(row):\n",
    "                if cell == 'S':\n",
    "                    self.initial_state = (j, i)\n",
    "        self.current_state = self.initial_state\n",
    "\n",
    "    def __repr__(self):\n",
    "        board = self.board.copy()\n",
    "        x, y = self.current_state\n",
    "        board.loc[y, x] += 'C'\n",
    "        return board.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def get_current_state(self) -> State:\n",
    "        return self.current_state\n",
    "\n",
    "    @staticmethod\n",
    "    def get_possible_actions(_state: Tuple[int, int]) -> List[Action]:\n",
    "        return [action for action in Action]\n",
    "\n",
    "    def get_reward(self) -> int:\n",
    "        x, y = self.current_state\n",
    "        try:\n",
    "            return int(self.board.loc[y, x])\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def do_action(self, action: Action) -> Tuple[int, State]:\n",
    "        x, y = self.current_state\n",
    "        x_, y_ = x, y\n",
    "        if action == Action.UP:\n",
    "            y_ = max(0, y - 1)\n",
    "        elif action == Action.DOWN:\n",
    "            y_ = min(self.num_rows - 1, y + 1)\n",
    "        elif action == Action.LEFT:\n",
    "            x_ = max(0, x - 1)\n",
    "        elif action == Action.RIGHT:\n",
    "            x_ = min(self.num_cols - 1, x + 1)\n",
    "        if self.board.loc[y_, x_] != '#':\n",
    "            self.current_state = x_, y_\n",
    "        return self.get_reward(), self.current_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self.get_reward() != 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e6bb7e7925fbf",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la definición del agente, genere un ambiente de `10x10` como se muestra a continuación.\n",
    "\n",
    "![evaluacion.png](https://raw.githubusercontent.com/FLAGlab/isis4222-rl/a502e264157729fcb8cc00d484e4a8e8e4734a15/week3/img/evaluacion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e18247d512848ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.706628Z",
     "start_time": "2024-08-25T18:25:57.698331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  1  2  3  4   5   6  7  8  9\n",
      "0  SC                             \n",
      "1                                 \n",
      "2      #  #  #  #       #  #  #   \n",
      "3               #                 \n",
      "4               #  -1             \n",
      "5               #  +1             \n",
      "6               #                 \n",
      "7               #  -1  -1         \n",
      "8                                 \n",
      "9                                 \n"
     ]
    }
   ],
   "source": [
    "env_board = [['S'] + [' '] * 9,\n",
    "             [' '] * 10,\n",
    "             [' ', '#', '#', '#', '#', ' ', '#', '#', '#', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', '-1', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', '+1', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', '-1', '-1', ' ', ' ', ' '],\n",
    "             [' '] * 10,\n",
    "             [' '] * 10\n",
    "             ]\n",
    "\n",
    "environment = Environment(env_board)\n",
    "\n",
    "print(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b2157eb4cbd464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.711811Z",
     "start_time": "2024-08-25T18:25:57.707472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (0, 1))\n",
      "    0  1  2  3  4   5   6  7  8  9\n",
      "0   S                             \n",
      "1   C                             \n",
      "2      #  #  #  #       #  #  #   \n",
      "3               #                 \n",
      "4               #  -1             \n",
      "5               #  +1             \n",
      "6               #                 \n",
      "7               #  -1  -1         \n",
      "8                                 \n",
      "9                                 \n"
     ]
    }
   ],
   "source": [
    "print(environment.do_action(Action.DOWN))\n",
    "print(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1ebfa6fec7b71",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2.\n",
    "Plantee el problema de MDP para cada una de las casillas. Especifique el estado de inicio, las transiciones y su probabilidad (suponiendo que todas las acciones sucede con probabilidad de 0.25) y los estados de fin con su recompensa.\n",
    "¿Cómo serían las recompensas esperadas para cada estado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b736602ca732bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:28:04.298059Z",
     "start_time": "2024-08-25T18:28:04.291814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "S = [(x, y) for x in range(environment.num_cols) for y in range(environment.num_rows)]\n",
    "A = [action for action in Action]\n",
    "\n",
    "\n",
    "# P(s_ | s, a) = P(s_, s, a)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def P(next_state: State, current_state: State, action: Action) -> float:\n",
    "    temp_environment = Environment(environment.board)\n",
    "    temp_environment.current_state = current_state\n",
    "    _, environment_next_state = temp_environment.do_action(action)\n",
    "    return int(environment_next_state == next_state)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def R(next_state: State):\n",
    "    temp_environment = Environment(environment.board)\n",
    "    temp_environment.current_state = next_state\n",
    "    return temp_environment.get_reward()\n",
    "\n",
    "\n",
    "print(P((1, 0), (1, 1), Action.UP))\n",
    "print(P((3, 0), (3, 3), Action.UP))\n",
    "print(R((1, 0)))\n",
    "print(R((3, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d801692120827",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3.\n",
    "Bajo la definción del problema anterior, suponga que cada acción tiene una probabilidad de éxito de 60%, con probabilidad de 30% se ejecutará la sigiente acción (en dirección de las manesillas del reloj) y con probabilidad de 10% no pasará nada. Bajo estas condiciones, ¿Cómo serían las recompensas esperadas para cada estado? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f51a015c08eec61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.721553Z",
     "start_time": "2024-08-25T18:25:57.718936Z"
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def P_2(next_state: State, current_state: State, action: Action) -> float:\n",
    "    action_index = A.index(action)\n",
    "    alternative_action = A[(action_index + 1) % len(A)]\n",
    "    return 0.6 * P(next_state, current_state, action) + 0.3 * P(next_state, current_state,\n",
    "                                                                alternative_action) + 0.1 * int(\n",
    "        next_state == current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b452809e833f20ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.820262Z",
     "start_time": "2024-08-25T18:25:57.722422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "s = (3, 3)\n",
    "a = Action.UP\n",
    "\n",
    "print(pd.DataFrame([\n",
    "    [P_2((x, y), s, a) for x in range(environment.num_cols)] for y in range(environment.num_rows)\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cc01e-d7a7-4cb5-b72b-2d2adf013c9b",
   "metadata": {},
   "source": [
    "\n",
    "### Task 4. \n",
    "Defina una situación de la vide real, de su escogencia, como un MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36661c91e623c9c",
   "metadata": {},
   "source": [
    "### Estrategia de mercadeo\n",
    "\n",
    "En una compañía se debe establecer una estrategia de mercadeo entre tres opciones. Cada estrategia tiene probabilidades distintas de atraer a un número particular de clientes; Sin embargo, las estrategias tienen un costo que afecta el precio del servicio dado por la compañía a los clientes que ya tiene. Mientras más alto es este precio es más probable que las personas decidan retirarse del servicio.\n",
    "\n",
    "- Estados (S): Los estados son el número de empleados que se tienen en un momento dado.\n",
    "- Acciones: las acciones posibles son las tres estrategias de mercadeo que la empresa tiene a su disposición más no ejecutar ninguna campaña.\n",
    "- Probabilidades (P): `P(s'|s,a) = SUM(P(empleados_perdidos|s,precio(a))*P(empleados_ganados|a)|s'=s-empleados_perdidos+empleados_ganados)`\n",
    "- Recompensa (R): Es igual a los ingresos de la empresa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
