{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de Monte Carlo\n",
    "\n",
    "En este ejercicio vamos a implementar la primera solución para los problemas de aprendizaje por refuerzo, los métodos de Monte Carlo. \n",
    "\n",
    "Recuerde que el método de Monte Carlo consiste en la colección de muestras calculando los valores para la secuencia completa de los estados hasta el estado final. Una vez se han coleccionado \"suficientes\" muestras, el valor de los estados se toma como el valor promedio de las muestras sobre las cuales apareció el estado.\n",
    "\n",
    "Para resolver problemas de aprendizaje por refuerzo utilizando el método de Monte Carlo crearemos un archivo `mcm.py`. Inicialmente utilizaremos este archivo para solucionar el problema sobre el ambiente de Gridworld (suponiendo un ruido de `0.25` para las acciones, es decir que la probabilidad de ejecutar la acción deseada es de 0.75 y una acción aleatoria (dividida en partes iguales) con probabilidad de 0.25).\n",
    "\n",
    "**Task 1**\n",
    "1.\tImplemente la classe `MCM` para solucionar Gridworld sin conocer los detalles del modelo de MDP para el problema. Es decir, en este caso, nuestro agente de `MCM` no tendrá acceso al `mdp` como era el caso para la iteración de valores o iteración de políticas.\n",
    "\n",
    "2. El comportamiento del agente (de Monte Carlo) esta dado por dos momentos. El proceso de recolección de muestras y el proceso de explotación de las mismas, es decir, el cálculo de la política del agente. Usted debe implementar el comportamiento del agente dado que, ejecutando episodios como muestras, sea capaz de calcular los valores para los estados.\n",
    "\n",
    "Para la implementación de `MCM` responda las siguientes preguntas. Tenga en cuenta que debe ejecutar su agente múltiples veces para poder observar el comportamiento (una sola instancia no nos puede llevar a ninguna conclución).\n",
    "Justifique su respuestas con análisis de la ejejcución y gráficas del comportamiento.\n",
    "1. ¿Cuantas muestras debe tomar el agente? Su implementación no debe utilizar este número como un parámetro o tenerlo como un factor predeterminado del agente.\n",
    "2. ¿Cómo se comparan los valores de `MCM` con respecto a los valores obtenidos en el ejercicio de iteración de valores `value_iteration`? ¿Porqué se da la diferencia si existe alguna, o porqué no existe ninguna diferencia?   \n",
    "3. ¿Cómo se compara la política obtenida utilizando `MCM` con respecto a la política obtenida en el jercicio de iteración de políticas `policy_iteration`? ¿Porqué se da la diferencia si existe alguna, o porqué no existe ninguna diferencia?\n",
    "4. ¿Cuál es el efecto de del factor de descuento sobre el método de Monte Carlo, calcule la solución de Gridworld con diferentes valores?\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:26:32.764499Z",
     "start_time": "2024-09-20T18:26:32.757893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from environment_world import EnvironmentWorld\n",
    "from mcm import MonteCarloAgent\n",
    "\n",
    "grid_world = EnvironmentWorld([\n",
    "    ['S'] + [' '] * 9,\n",
    "    [' '] * 10,\n",
    "    [' ', '#', '#', '#', '#', ' ', '#', '#', '#', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', '-1', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', '+1', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "    [' ', ' ', ' ', ' ', '#', '-1', '-1', ' ', ' ', ' '],\n",
    "    [' '] * 10,\n",
    "    [' '] * 10\n",
    "], noise=0.25)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:38:10.965706Z",
     "start_time": "2024-09-20T18:38:10.963348Z"
    }
   },
   "cell_type": "code",
   "source": "montecarlo_agent = MonteCarloAgent(grid_world, discount_factor=0.9, initial_epsilon=0.9, epsilon_decay=0.999)",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:39:12.032729Z",
     "start_time": "2024-09-20T18:38:12.788796Z"
    }
   },
   "cell_type": "code",
   "source": "montecarlo_agent.learn(max_episodes=1000, convergence_check_frequency=10, convergence_patience=1)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|██████████| 1000/1000 [00:59<00:00, 16.88it/s]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:41:49.443685Z",
     "start_time": "2024-09-20T18:41:49.438274Z"
    }
   },
   "cell_type": "code",
   "source": "montecarlo_agent.print_policy()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1      2      3      4      5      6      7      8      9\n",
      "0   down  right  right  right  right  right  right   down  right   down\n",
      "1  right   down     up   left     up     up  right  right  right   down\n",
      "2  right   None   None   None   None     up   None   None   None   down\n",
      "3   left   left   left   down   None  right  right   down     up   down\n",
      "4     up     up     up     up   None   None   down   down   down   down\n",
      "5     up   left   left   left   None   None   left   left   left   left\n",
      "6     up     up     up     up   None     up   left     up     up     up\n",
      "7   left   left     up     up   None   None   None  right     up     up\n",
      "8     up     up   left     up   left   down   down  right  right   down\n",
      "9   down     up   left   left   left   left  right  right   down  right\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:01:53.848392Z",
     "start_time": "2024-09-20T18:01:53.845128Z"
    }
   },
   "cell_type": "code",
   "source": "montecarlo_agent.Q[(1, 1)]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {<Action.UP: 'up'>: 0.007817332347623789,\n",
       "             <Action.RIGHT: 'right'>: 0.0008116626848326855,\n",
       "             <Action.DOWN: 'down'>: 0.0005960036322950394,\n",
       "             <Action.LEFT: 'left'>: 0.0010282757367423463})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
