{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0a6b89-5f5b-4706-8da8-c8d00ff69870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gridworld y su soluci칩n como MDPs\n",
    "\n",
    "En este trabajo definiremos el ambiente de Gridworld y su soluci칩n como un MDP.\n",
    "Gridworld es un ambiente cl치sico de prueba dentro del aprendizaje por refuerzo. Durante este taller definiremos el modelo b치sico del ambiente, que extenderemos incrementalmente de acuerdo a las necesidades del algoritmo de soluci칩n.\n",
    "\n",
    "## Ambiente 游깵\n",
    "\n",
    "El ambiente de ridworld se define como una cuadricula de `nxm`. El ambiente tiene obstaculos, es decir casillas por las cuales no puede pasar el agente. Al chocar con un obstaculo, el agente se mantiene terminar칤a en el mismo estado inicial. Adem치s, el ambiente tiene una casilla de inicio, y algunas casillas de salida. Un ejemplo del ambiente para el caso `3x4` se muestra a continuaci칩n.\n",
    "\n",
    "![gridworld.png](https://raw.githubusercontent.com/FLAGlab/isis4222-rl/a502e264157729fcb8cc00d484e4a8e8e4734a15/week3/img/gridworld.png)\n",
    "\n",
    "En este ejemplo del ambiente el agente comienza en la casilla inferior izquierda y tiene como objetivo llegar a la casilla de salida verde, con recompensa 1. La otra casilla de salida, tiene recompensa -1.\n",
    "\n",
    "\n",
    "### Task 1.\n",
    "#### 쮺칩mo podemos codificar el ambiente?\n",
    "\n",
    "De una definici칩n completa del ambiente, como una clase de python llamada `Environment`, estableciendo:\n",
    "1. Un atributo que define la cuadr칤cula (`board`). El ambiente recibir치 una matriz como par치metro describiendo la cuadr칤cula en el momento de su creaci칩n. Definiremos las casillas por las que puede pasar el agente como casillas vacias, las casillas por las que no puede pasar el agente con un valor none `None` y las casillas de salida con el valor asociado a la recompensa definidas para cada una de ellas.\n",
    "2. Un atributo `nrows` para almacenar la cantidad de filas de la cuadr칤cula.\n",
    "3. Un atributo `ncols` para almacenar la cantidad de columnas de la cuadr칤cula.\n",
    "4. Un atributo `initial_state` para almacenar el estado inicial del agente dentro del ambiente.\n",
    "5. Un atributo con el estado actual (`current_state`) en el que se encuentra el agente. El valor de `current_state` se definir치 como una tupla \n",
    "\n",
    "Un ejemplo de la definici칩n del tablero para el caso de 5x5 de la figura anterior se da a continuaci칩n.\n",
    "```\n",
    "board = [['', ' ', ' ',  '+1'],\n",
    "         [' ', '#', ' ',  '-1'],\n",
    "         ['S', ' ', ' ', ' ']]\n",
    "```\n",
    "En el ejemplo `S` denota el estado inicial y `'#'` la casilla prohibida (manejaremos esta convenci칩n para todos los ambientes de gridworld).\n",
    "\n",
    "#### Comportamiento del ambeinte\n",
    "\n",
    "Una vez definido el ambiente definimos su comportamiento. Para ello requerimos los siguientes m칠todos:\n",
    "1. `get_current_state` que no recibe par치metros y retorna el estado actual (la casilla donde se encuentra el agente)\n",
    "2. `get_posible_actions` que recibe el estado actual del agente como par치metro y retorna las acciones disponibles para dicho estado. Las acciones estar치n dadas por su nombre (`'up', 'down', 'left', 'right'`). Como convenci칩n definiremos que el agente siempre puede moverse en todas las direcciones, donde un movimiento en direcci칩n de un obst치culo o los l칤mites del ambiente no tienen ning칰n efecto visible en la posici칩n del agente.\n",
    "3. `do_action` que recibe como par치metro la acci칩n a ejecutar y retorna el valor de la recompensa y el nuevo estado del agente, como un pareja `reward, new_state`\n",
    "4. `reset` que no recibe par치metros y restablece el ambiente a su estado inicial.\n",
    "5. `is_terminal` que no recibe par치metros y determina si el agente est치 en el estado final o no. En nuestro caso, el estado final estar치 determinado por las casillas de salida (i.e., con un valor definido).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855dbb21ff626c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.697151Z",
     "start_time": "2024-08-25T18:25:57.342651Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    UP = 'up'\n",
    "    RIGHT = 'right'\n",
    "    DOWN = 'down'\n",
    "    LEFT = 'left'\n",
    "\n",
    "\n",
    "State = Tuple[int, int]\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, board: List[List[str]]):\n",
    "        self.board = pd.DataFrame(board)\n",
    "        self.num_rows = len(board)\n",
    "        self.num_cols = len(board[0])\n",
    "        self.initial_state = (0, 0)\n",
    "        for i, row in self.board.iterrows():\n",
    "            for j, cell in enumerate(row):\n",
    "                if cell == 'S':\n",
    "                    self.initial_state = (j, i)\n",
    "        self.current_state = self.initial_state\n",
    "\n",
    "    def __repr__(self):\n",
    "        board = self.board.copy()\n",
    "        x, y = self.current_state\n",
    "        board.loc[y, x] += 'C'\n",
    "        return board.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def get_current_state(self) -> State:\n",
    "        return self.current_state\n",
    "\n",
    "    @staticmethod\n",
    "    def get_possible_actions(_state: Tuple[int, int]) -> List[Action]:\n",
    "        return [action for action in Action]\n",
    "\n",
    "    def get_reward(self) -> int:\n",
    "        x, y = self.current_state\n",
    "        try:\n",
    "            return int(self.board.loc[y, x])\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def do_action(self, action: Action) -> Tuple[int, State]:\n",
    "        x, y = self.current_state\n",
    "        x_, y_ = x, y\n",
    "        if action == Action.UP:\n",
    "            y_ = max(0, y - 1)\n",
    "        elif action == Action.DOWN:\n",
    "            y_ = min(self.num_rows - 1, y + 1)\n",
    "        elif action == Action.LEFT:\n",
    "            x_ = max(0, x - 1)\n",
    "        elif action == Action.RIGHT:\n",
    "            x_ = min(self.num_cols - 1, x + 1)\n",
    "        if self.board.loc[y_, x_] != '#':\n",
    "            self.current_state = x_, y_\n",
    "        return self.get_reward(), self.current_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.initial_state\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        return self.get_reward() != 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e6bb7e7925fbf",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la definici칩n del agente, genere un ambiente de `10x10` como se muestra a continuaci칩n.\n",
    "\n",
    "![evaluacion.png](https://raw.githubusercontent.com/FLAGlab/isis4222-rl/a502e264157729fcb8cc00d484e4a8e8e4734a15/week3/img/evaluacion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e18247d512848ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.706628Z",
     "start_time": "2024-08-25T18:25:57.698331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  1  2  3  4   5   6  7  8  9\n",
      "0  SC                             \n",
      "1                                 \n",
      "2      #  #  #  #       #  #  #   \n",
      "3               #                 \n",
      "4               #  -1             \n",
      "5               #  +1             \n",
      "6               #                 \n",
      "7               #  -1  -1         \n",
      "8                                 \n",
      "9                                 \n"
     ]
    }
   ],
   "source": [
    "env_board = [['S'] + [' '] * 9,\n",
    "             [' '] * 10,\n",
    "             [' ', '#', '#', '#', '#', ' ', '#', '#', '#', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', '-1', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', '+1', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', ' ', ' ', ' ', ' ', ' '],\n",
    "             [' ', ' ', ' ', ' ', '#', '-1', '-1', ' ', ' ', ' '],\n",
    "             [' '] * 10,\n",
    "             [' '] * 10\n",
    "             ]\n",
    "\n",
    "environment = Environment(env_board)\n",
    "\n",
    "print(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b2157eb4cbd464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.711811Z",
     "start_time": "2024-08-25T18:25:57.707472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (0, 1))\n",
      "    0  1  2  3  4   5   6  7  8  9\n",
      "0   S                             \n",
      "1   C                             \n",
      "2      #  #  #  #       #  #  #   \n",
      "3               #                 \n",
      "4               #  -1             \n",
      "5               #  +1             \n",
      "6               #                 \n",
      "7               #  -1  -1         \n",
      "8                                 \n",
      "9                                 \n"
     ]
    }
   ],
   "source": [
    "print(environment.do_action(Action.DOWN))\n",
    "print(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1ebfa6fec7b71",
   "metadata": {},
   "source": [
    "\n",
    "### Task 2.\n",
    "Plantee el problema de MDP para cada una de las casillas. Especifique el estado de inicio, las transiciones y su probabilidad (suponiendo que todas las acciones sucede con probabilidad de 0.25) y los estados de fin con su recompensa.\n",
    "쮺칩mo ser칤an las recompensas esperadas para cada estado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b736602ca732bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:28:04.298059Z",
     "start_time": "2024-08-25T18:28:04.291814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "S = [(x, y) for x in range(environment.num_cols) for y in range(environment.num_rows)]\n",
    "A = [action for action in Action]\n",
    "\n",
    "\n",
    "# P(s_ | s, a) = P(s_, s, a)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def P(next_state: State, current_state: State, action: Action) -> float:\n",
    "    temp_environment = Environment(environment.board)\n",
    "    temp_environment.current_state = current_state\n",
    "    _, environment_next_state = temp_environment.do_action(action)\n",
    "    return int(environment_next_state == next_state)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def R(next_state: State):\n",
    "    temp_environment = Environment(environment.board)\n",
    "    temp_environment.current_state = next_state\n",
    "    return temp_environment.get_reward()\n",
    "\n",
    "\n",
    "print(P((1, 0), (1, 1), Action.UP))\n",
    "print(P((3, 0), (3, 3), Action.UP))\n",
    "print(R((1, 0)))\n",
    "print(R((3, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d801692120827",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3.\n",
    "Bajo la definci칩n del problema anterior, suponga que cada acci칩n tiene una probabilidad de 칠xito de 60%, con probabilidad de 30% se ejecutar치 la sigiente acci칩n (en direcci칩n de las manesillas del reloj) y con probabilidad de 10% no pasar치 nada. Bajo estas condiciones, 쮺칩mo ser칤an las recompensas esperadas para cada estado? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f51a015c08eec61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.721553Z",
     "start_time": "2024-08-25T18:25:57.718936Z"
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def P_2(next_state: State, current_state: State, action: Action) -> float:\n",
    "    action_index = A.index(action)\n",
    "    alternative_action = A[(action_index + 1) % len(A)]\n",
    "    return 0.6 * P(next_state, current_state, action) + 0.3 * P(next_state, current_state,\n",
    "                                                                alternative_action) + 0.1 * int(\n",
    "        next_state == current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b452809e833f20ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T18:25:57.820262Z",
     "start_time": "2024-08-25T18:25:57.722422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "s = (3, 3)\n",
    "a = Action.UP\n",
    "\n",
    "print(pd.DataFrame([\n",
    "    [P_2((x, y), s, a) for x in range(environment.num_cols)] for y in range(environment.num_rows)\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cc01e-d7a7-4cb5-b72b-2d2adf013c9b",
   "metadata": {},
   "source": [
    "\n",
    "### Task 4. \n",
    "Defina una situaci칩n de la vide real, de su escogencia, como un MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36661c91e623c9c",
   "metadata": {},
   "source": [
    "### Estrategia de mercadeo\n",
    "\n",
    "En una compa침칤a se debe establecer una estrategia de mercadeo entre tres opciones. Cada estrategia tiene probabilidades distintas de atraer a un n칰mero particular de clientes; Sin embargo, las estrategias tienen un costo que afecta el precio del servicio dado por la compa침칤a a los clientes que ya tiene. Mientras m치s alto es este precio es m치s probable que las personas decidan retirarse del servicio.\n",
    "\n",
    "- Estados (S): Los estados son el n칰mero de empleados que se tienen en un momento dado.\n",
    "- Acciones: las acciones posibles son las tres estrategias de mercadeo que la empresa tiene a su disposici칩n m치s no ejecutar ninguna campa침a.\n",
    "- Probabilidades (P): `P(s'|s,a) = SUM(P(empleados_perdidos|s,precio(a))*P(empleados_ganados|a)|s'=s-empleados_perdidos+empleados_ganados)`\n",
    "- Recompensa (R): Es igual a los ingresos de la empresa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
