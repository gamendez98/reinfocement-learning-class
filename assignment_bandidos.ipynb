{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de Bandidos de K-brazos #\n",
    "\n",
    "En este ejercicio estableceremos las tres implementaciones de bandidos de K-brazos. Estos son:\n",
    "1. Bandido greedy\n",
    "2. Bandido con &epsilon;-decay\n",
    "3. Bandido con reocmpensas no estacionarias\n",
    "\n",
    "Por defecto, utilizaremos bandidos con 10 brazos, sin embargo este será un valor parametrizable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bandido greedy ##\n",
    "\n",
    "La definición de los bandidos consiste en crear un agente, como una clase de Python, `Bandit`. Inicialmente crearemos un bandido con una cantidad de brazos dado por parámetro (por defecto utilizaremos 10 brazos). \n",
    "Cada uno de los brazos del bandido (definidos en una lista `arms` como un atributo de la clase) tiene su propia recompensa, la cual asignaremos como un número flotante aleatorio en el rango `[-3,3]` (utilizando la función `random.uniform(-3,3)` de la librería random). Adicionalmente, definimos un atributo de la clase (`reward`) para mantener la recompensa cumulativa del bandido.\n",
    "Inicialmente nuestro bandido tiene dos funciones `choose_arm` y `run`.\n",
    "\n",
    "- `choose_arm` no tienen parámetros. Esta función se encarga de seleccionar un brazo a ejecutar (de forma aleatoria en caso de empate), y retorna la recompensa recibida de acuerdo a la fórmula:\n",
    "\n",
    "![title](https://raw.githubusercontent.com/FLAGlab/isis4222-rl/c0d6b663b8ce1dd5057c0c06e30ccde46b5409c6/week2/img/bandit_formula.png)\n",
    "\n",
    "Retornando la recompensa promedio obtenida por el agente al escoger el brazo dado.\n",
    "\n",
    "Para poder realizar este cálculo, debemos llevar la cuenta de la cantidad de veces que ha sido escogido cada brazo y la recompensa acumulada de cada brazo. Esta información la definimos como atributos de la clase, en las variables `occurrences` y `cumulative_rewards`,  respectivamente. Ambos atributos se definen como una lista de tamaño número de brazos del bandido inicialmente en 0.\n",
    "\n",
    "El cálculo de la recompensa primero actualiza las ocurrencias de escoger el brazo seleccionado aleatoriamente, `arm`. Además debemos actualizar la recompensa cumulativa del brazo `arm`, sumando la recompensa correspondiente al brazo (obtenida de la lista de brazos). Finalmente, calculamos Q<sub>t</sub>(a) como la recompensa cumulativa sobre las ocurrencias para el brazo dado y retornamos dicho valor. La recompensa global del agente se actualiza con el valor calculado.\n",
    "\n",
    "- `run` no recibe ningún parámetro. Esta función define la cantidad de episodios para los cuales vamos a calcular la recompensa del agente (1000 en nuestro caso)\n",
    "Luego creamos un bandido (`bandit`) y una variable para almacenar la recompensa obtenida en cada iteración (`expected_reward`).\n",
    "Para cada uno de los episodios, ejecutamos el bandido llamando la función choose_arm y guardamos el resultado en la lista de recompensas esperadas.\n",
    "Finalmente retornamos la lista de recompensas para cada iteración (`expected_reward`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:56:18.286076Z",
     "start_time": "2024-08-13T15:56:18.281924Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class GreedyBandit:\n",
    "    def __init__(self, n_arms=10):\n",
    "        self.arms = [random.uniform(-3, 3) for _ in range(n_arms)]\n",
    "        self.reward = 0\n",
    "        self.expected_rewards = [0] * n_arms\n",
    "        self.cumulative_rewards = [0] * n_arms\n",
    "        self.occurrences = [0] * n_arms\n",
    "\n",
    "    def choose_arm(self):\n",
    "        best_reward = max(self.expected_rewards)\n",
    "        candidate_arms = [i for i, reward in enumerate(self.expected_rewards) if reward == best_reward]\n",
    "        chosen_arm = random.choice(candidate_arms)\n",
    "        reward = self.arms[chosen_arm]\n",
    "        self.cumulative_rewards[chosen_arm] += reward\n",
    "        self.occurrences[chosen_arm] += 1\n",
    "        self.expected_rewards[chosen_arm] = self.cumulative_rewards[chosen_arm] / self.occurrences[chosen_arm]\n",
    "        return self.expected_rewards[chosen_arm]\n",
    "\n",
    "    def run(self, episodes=10):\n",
    "        expected_reward = [0 for _i in range(episodes)]\n",
    "        for i in range(0, episodes):\n",
    "            expected_reward[i] = self.choose_arm()\n",
    "        return expected_reward"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:56:20.055989Z",
     "start_time": "2024-08-13T15:56:20.049450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "greedy_bandit = GreedyBandit()\n",
    "print(greedy_bandit.run(episodes=10))\n",
    "\n",
    "print(greedy_bandit.expected_rewards)\n",
    "print(greedy_bandit.occurrences)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4442293484141846, 1.4442293484141846, 1.4442293484141846, 1.4442293484141846, 1.4442293484141846, 1.4442293484141846, 1.4442293484141846, 1.4442293484141846, 1.4442293484141846, 1.4442293484141846]\n",
      "[0, 0, 0, 1.4442293484141846, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 10, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bandidos con &epsilon;-decay ##\n",
    "\n",
    "Implemente el algoritmos de los bandidos ahora utilizando la estrategia de epsilon decay para mediar entre la exploración y la explotación de los bandidos.\n",
    "Recuerde que esta nueva estrategia se basa en escoger la acción con mayor recompensa esperada Q<sub>n+1</sub> con probabilidad &epsilon; y una acción aleatoria con probabilidad 1-&epsilon;.\n",
    "\n",
    "Para ello, modifique el bandido implementado anteriormente (`Bandit`) agregando un nuevo atributo `epsilon` con un valor de 0.1 por defecto. Además agregue una nueva función `choose_arm` que retorna la acción a tomar, siguiendo las especificaciones del valor &epsilon;. La función `choose_arm` será utilizada desde la función `run` para escoger el brazo."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:56:47.319529Z",
     "start_time": "2024-08-13T15:56:47.315050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class EpsilonBandit:\n",
    "    def __init__(self, n_arms=10, epsilon=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.arms = [random.uniform(-3, 3) for _ in range(n_arms)]\n",
    "        self.reward = 0\n",
    "        self.expected_rewards = [0] * n_arms\n",
    "        self.cumulative_rewards = [0] * n_arms\n",
    "        self.occurrences = [0] * n_arms\n",
    "\n",
    "    def choose_arm(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            best_reward = max(self.expected_rewards)\n",
    "            candidate_arms = [i for i, reward in enumerate(self.expected_rewards) if reward == best_reward]\n",
    "            chosen_arm = random.choice(candidate_arms)\n",
    "        else:\n",
    "            chosen_arm = random.randrange(0, len(self.arms))\n",
    "        reward = self.arms[chosen_arm]\n",
    "        self.cumulative_rewards[chosen_arm] += reward\n",
    "        self.occurrences[chosen_arm] += 1\n",
    "        self.expected_rewards[chosen_arm] = self.cumulative_rewards[chosen_arm] / self.occurrences[chosen_arm]\n",
    "        return self.expected_rewards[chosen_arm]\n",
    "\n",
    "    def run(self, episodes = 10):\n",
    "        expected_reward = [0 for _i in range(episodes)]\n",
    "        for i in range(0, episodes):\n",
    "            expected_reward[i] = self.choose_arm()\n",
    "        return expected_reward"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:57:09.273180Z",
     "start_time": "2024-08-13T15:57:09.270759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epsilon_bandit = EpsilonBandit()\n",
    "print(epsilon_bandit.run())\n",
    "\n",
    "print(epsilon_bandit.expected_rewards)\n",
    "print(epsilon_bandit.occurrences)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2984796063440447, 1.5348025937524987, 1.7313220600698536, 1.7313220600698536, 1.7313220600698536, 1.7313220600698536, 1.7313220600698536, 1.7313220600698536, 1.7313220600698536, 1.7313220600698536]\n",
      "[1.5348025937524987, 0, 0, 0, 0, 0, 0, -2.2984796063440447, 1.7313220600698536, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 8, 0]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bandido con recompensas no estacionarias #\n",
    "\n",
    "Ahora retomaremos el bandido del punto 2. para permitir que las recompensas sean no estacionarias.\n",
    "Para lograr este objetivo, debemos realizar las siguientes modificaciones sobre el bandido.\n",
    "1. Agregar un atributo con los valores iniciales de recompensa para cada brazo. Con este proposito podemos utilizar la el atributo `arms`, que tenia la recompensa de cada brazo.\n",
    "Por defecto le daremos un valor inicial a cada brazo, como un entero aleatorio entre 1 y el número de brazos.\n",
    "2. Debemos introducir un atrobuto &alpha; para el bandido que tendra como valor por defecto `0.1`\n",
    "3. Dentro de la función `expected_reward`, debemos modificar la recompensa de cada episodio para cada brazo. Para ello utilizaremos una función sinusoidal que nos da la recompensa actual como el seno de la multiplicación del paso actual con el brazo. Todo esto como un factor de la recompensa inicial del brazo.\n",
    "4. Finalmente, dentro de la función `expected_reward`, debemos realizar el cálculo de la recompensa esperada según la fórmula de las recompensas no estacionarias y almacenarla en el atributo `cumulative_rewards` para cada brazo. \n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T16:08:49.963475Z",
     "start_time": "2024-08-13T16:08:49.956251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class VariableBandit:\n",
    "    def __init__(self, n_arms=10, alfa=0.1):\n",
    "        self.alfa = alfa\n",
    "        self.arms = [random.uniform(-3, 3) for _ in range(n_arms)]\n",
    "        self.reward = 0\n",
    "        self.expected_rewards = [0] * n_arms\n",
    "        self.current_episode = 1\n",
    "\n",
    "    def choose_arm(self):\n",
    "        best_reward = max(self.expected_rewards)\n",
    "        candidate_arms = [i for i, reward in enumerate(self.expected_rewards) if reward == best_reward]\n",
    "        chosen_arm = random.choice(candidate_arms)\n",
    "        reward = math.sin(self.arms[chosen_arm] * self.current_episode)\n",
    "        self.current_episode += 1\n",
    "        self.expected_rewards[chosen_arm] = self.expected_rewards[chosen_arm] + self.alfa * (reward - self.expected_rewards[chosen_arm])\n",
    "        return self.expected_rewards[chosen_arm]\n",
    "\n",
    "    def run(self, episodes = 10):\n",
    "        expected_reward = [0 for _i in range(episodes)]\n",
    "        for i in range(0, episodes):\n",
    "            expected_reward[i] = self.choose_arm()\n",
    "        return expected_reward"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T16:09:31.055638Z",
     "start_time": "2024-08-13T16:09:31.051316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "variable_bandit = VariableBandit()\n",
    "print(variable_bandit.run())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09637363762629368, 0.09528269041586296, -0.009079715351970497, 0.09634439073049131, 0.1611120394643541, 0.1819594276997673, 0.15557849838039764, 0.08839737215214359, -0.004748556293294459, 0.09542909921508479]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio #\n",
    "\n",
    "Una vez definidos los tres algoritmos de bandidos, debe ejecutar cada uno de ellos sobre un ambiente de pruebas (pueden usar este ejemplo de base <a href=\"https://github.com/FLAGlab/isis4222-rl/blob/master/week2/graph.py\" >Ambiente pruebas</a>) que permita comparar los algoritmos. Dicha ejecución sera la base del análisis de los algoritmos.\n",
    "\n",
    "Entregue:\n",
    "- La implementación de cada uno de los algoritmos de bandidos como notebooks/archivos independientes\n",
    "- Un pdf con:\n",
    "  - El análisis del comportamiento de cada uno de los algortmos de bandidos \n",
    "  - El análisis comparativo de los tres algoritmos de bandidos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
