{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por diferencia temporal (TD)\n",
    "\n",
    "En este ejercicio vamos a implementar el método de diferencia temporal para para resolver MDPs con transiciones y recompensas desconocidas. \n",
    "\n",
    "El método de TD se basa en el cálculo de los valores para los estados de acuerdo con la fórmula:\n",
    "\n",
    "$V^\\pi(s) \\leftarrow (1-\\alpha)V^\\pi(s) + \\alpha[R(s, \\pi(s), s') + \\gamma V^\\pi(s')]$\n",
    "\n",
    "donde $\\alpha$ corresponde a la taza de aprendizaje.\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "Para implementar TD definimos `td_learning.py` como una extensión del ambiente de Gridworld. Dentro de esta extensión debemos asegurarnos que:\n",
    "  - Seguimos una política, dada como un parámetro del ambiente.\n",
    "  - Cada paso de la muestra ejecuta la política para el estado actual, obteniendo un estado de llegada y una recompensa. Tenga en cuenta que las acciones no son determinísticas y la ejecución de cada acción depende de un factor de ruido (en el caso de Gridworld, tomaremos un factor de ruido de 0.2 para las acciones abajo e izquierda y 0.3 para las acciones arriba y derecha, desconocida para el agente). Por ejemplo, el agente tiene una probabilidad de 0.8 de moverse a la izquierda y abajo y terminar en el estado correspondiente y probabilidad de 0.2 de terminar en cualquiera de las otras tres direcciones.\n",
    "  - A partir de los valores obtenidos de diferentes muestras, obtenga una nueva política.\n",
    "  - Utilice una taza de aprendizaje de `0.7`\n",
    "\n",
    "Responda las preguntas\n",
    "1. ¿Cuántas iteraciones son necesarias para que la política de las muestras se estabilice?\n",
    "2. ¿Cómo se compara la política obtenida con la calculada utilizando iteración de valores o iteración de políticas? ¿Existe alguna diferencia? ¿Porqué?\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:00:35.143480Z",
     "start_time": "2024-10-03T00:00:34.954649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from assignment_td_sarsa.environment_world import EnvironmentWorld, Action\n",
    "from assignment_td_sarsa.td_learning import TDLearning\n",
    "\n",
    "cliff_world = EnvironmentWorld([\n",
    "    ['-1'] * 12,\n",
    "    ['-1'] * 12,\n",
    "    ['-1'] * 12,\n",
    "    ['S'] + ['-100'] * 10 + ['1000']\n",
    "],\n",
    "    terminal_states=[(x, 3) for x in range(1, 12)], action_noise={\n",
    "        Action.UP: 0.3,\n",
    "        Action.DOWN: 0.2,\n",
    "        Action.LEFT: 0.2,\n",
    "        Action.RIGHT: 0.3\n",
    "    })"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:00:36.433686Z",
     "start_time": "2024-10-03T00:00:36.427196Z"
    }
   },
   "cell_type": "code",
   "source": "cliff_world",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     10    11\n",
       "0  -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "1  -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "2  -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "3  SC  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:00:39.083861Z",
     "start_time": "2024-10-03T00:00:39.081269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from random import choice\n",
    "\n",
    "actions = cliff_world.ACTIONS\n",
    "\n",
    "td_learning = TDLearning(cliff_world , lambda s: choice(actions), 0.7, 0.96)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:02:36.105628Z",
     "start_time": "2024-10-03T00:00:41.581306Z"
    }
   },
   "cell_type": "code",
   "source": "td_learning.iterate_learning(num_steps=10000000)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [01:54<00:00, 87323.64it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:03:15.552971Z",
     "start_time": "2024-10-03T00:03:15.544922Z"
    }
   },
   "cell_type": "code",
   "source": "td_learning.print_values()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0          1          2          3          4          5   \\\n",
      "0 -51.313448 -59.839199 -56.088128 -63.624704 -66.599113 -60.012854   \n",
      "1 -77.250012 -82.625253 -63.821178 -68.850574 -79.668051 -76.597852   \n",
      "2 -66.946054 -88.049476 -90.139071 -74.394359 -95.987255 -91.962725   \n",
      "3 -72.640919   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "\n",
      "          6          7          8          9           10          11  \n",
      "0 -69.946293 -51.569629 -34.906770  -7.087166   28.889009   55.659964  \n",
      "1 -60.778289 -57.673284 -51.226461  36.371327  101.248186  704.372941  \n",
      "2 -72.578859 -57.268722 -88.678313 -39.761121  212.872839  253.459005  \n",
      "3   0.000000   0.000000   0.000000   0.000000    0.000000    0.000000  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
