{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA: State-Action-Reward-State-Action\n",
    "\n",
    "En este ejercicio vamos a implementar el algoritmo de SARSA como un ejemplo de los métodos on-policy para el aprendizaje por refuerzo. Esto es, crear agentes que aprenden a alcanzar un objetivo específico.\n",
    "\n",
    "El método de SARSA se basa en el cálculo de los q-valores utilizando los valores calculados para el estado de llegada siguiendo la fórmula de actualización de los q-valores:\n",
    "\n",
    "$Q(s,a) \\leftarrow (1-\\alpha)Q(s,a) + \\alpha[R(s) + \\gamma Q(s',a')] $\n",
    "\n",
    "Para implementar SARSA definiremos un agente, `sarsa_agent.py` el cual utilizaremos para interactuar con el ambiente de Gridworld.\n",
    "\n",
    "#### Task 1\n",
    "1.\tImplemente la classe `SARSA` con cinco atributos:\n",
    "    - `epsilon` que corresponde a la estrategia de aprendizaje $\\epsilon$-greedy, `0.9` por defecto.\n",
    "    - `gamma` que corresponde al factor de decuento a utilizar, `0.96` por defecto.\n",
    "    - `alpha` que corresponde a la taza de aprendizajem `0.81` por defecto.\n",
    "    - `Q` que almacena los q-valores del agente.\n",
    "    - `env` que es una referencia al ambiente.\n",
    "\n",
    "2. El comportamiento del agente (la interacción con el ambiente) esta dado por los métodos:\n",
    "    - `choose_action` que recibe un estado como parámetro y retorna la acción a ejecutar para dicho estado siguiendo una estrategia $\\epsilon$-greedy.\n",
    "    - `action_function` que recibe como parámetro los componentes de SARSA (estado1, acción1, recompensa, estado2, acción2) y calcula el q-valor `Q(estdo1, acción1)`.\n",
    "\n",
    "3. La interacción entre el agente y el ambiente inicia desde el ambiente, que ejecuta cada interacción de SARSA para cada episodio.\n",
    "(1) La interacción comienza decidiendo la acción a tomar para el estado actual (la cual esta dada por el agente), (2) luego debemos ejecutar la acción, obteniendo el estado de llegada y la recompensa de ejecutar dicha acción, (3) luego calculamos la acción a tomar para el estado de llegada, (4) por último calculamos el q-valor definido por la función de las acciones.\n",
    "\n",
    "#### Task 2\n",
    "Implemente el ambiente de cliff-walk (basado en el ambiente de Gridworld utilizdo anteriormente) y resulevalo utilizando el método de SARSA.\n",
    "Recuerde que en este ambiente la recompensa por caer al barranco es de -100 y la recompensa de cada paso es -1.\n",
    "Para la ejecución vamos a suponer acciones determinísticas.\n",
    "\n",
    "![cliff-walk](cliff-walk.png)\n",
    "\n",
    "Además responda las siguientes preguntas\n",
    "1. ¿Cuál es el comportamiento del agente si utilizamos un factor de descuento de 1?\n",
    "2. ¿Cómo podemos minimizar la trayectoria del agente entre el estado inicial y el estado de llegada?\n",
    "\n",
    "Justifique sus respuestas con ejecuciones reales del agente."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:13:21.442123Z",
     "start_time": "2024-10-07T20:13:21.438284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from assignment_td_sarsa.environment_world import EnvironmentWorld, Action\n",
    "from assignment_td_sarsa.sarsa_agent import SarsaAgent\n",
    "\n",
    "cliff_world = EnvironmentWorld([\n",
    "    ['-1'] * 12,\n",
    "    ['-1'] * 12,\n",
    "    ['-1'] * 12,\n",
    "    ['-1'] + ['-100'] * 10 + ['1000']\n",
    "],\n",
    "    terminal_states=[(x, 3) for x in range(1, 12)],\n",
    "    initial_state=(0, 3))"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:13:24.442791Z",
     "start_time": "2024-10-07T20:13:24.434772Z"
    }
   },
   "cell_type": "code",
   "source": "cliff_world",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9     10    11\n",
       "0   -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "1   -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "2   -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1    -1\n",
       "3  -1C  -100  -100  -100  -100  -100  -100  -100  -100  -100  -100  1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:13:25.833650Z",
     "start_time": "2024-10-07T20:13:25.828479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sarsa_agent = SarsaAgent(\n",
    "    world=cliff_world,\n",
    "    learning_rate=0.81,\n",
    "    discount_factor=0.96,\n",
    "    epsilon=0.9\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:13:40.260449Z",
     "start_time": "2024-10-07T20:13:28.022848Z"
    }
   },
   "cell_type": "code",
   "source": "sarsa_agent.iterate_learning(num_steps=1000000)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:12<00:00, 81760.00it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:13:46.046140Z",
     "start_time": "2024-10-07T20:13:46.040869Z"
    }
   },
   "cell_type": "code",
   "source": "sarsa_agent.print_policy()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0      1      2      3      4      5      6      7      8      9   \\\n",
      "0   down  right   down   down  right   down   down   down  right  right   \n",
      "1  right  right  right  right  right  right   down  right  right   down   \n",
      "2  right  right  right  right  right  right  right  right  right  right   \n",
      "3     up   None   None   None   None   None   None   None   None   None   \n",
      "\n",
      "      10    11  \n",
      "0   down  down  \n",
      "1  right  down  \n",
      "2  right  down  \n",
      "3   None  None  \n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:14:07.477474Z",
     "start_time": "2024-10-07T20:14:07.474466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sarsa_agent = SarsaAgent(\n",
    "    world=cliff_world,\n",
    "    learning_rate=0.81,\n",
    "    discount_factor=1,\n",
    "    epsilon=0.9\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:14:22.142920Z",
     "start_time": "2024-10-07T20:14:09.650438Z"
    }
   },
   "cell_type": "code",
   "source": "sarsa_agent.iterate_learning(num_steps=1000000)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:12<00:00, 80091.88it/s]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:14:24.936781Z",
     "start_time": "2024-10-07T20:14:24.930300Z"
    }
   },
   "cell_type": "code",
   "source": "sarsa_agent.print_policy()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0      1      2      3      4      5      6      7      8      9   \\\n",
      "0  right  right  right  right  right   down  right  right  right  right   \n",
      "1  right  right  right   down  right  right  right  right   down   down   \n",
      "2  right     up  right  right  right  right  right  right  right  right   \n",
      "3     up   None   None   None   None   None   None   None   None   None   \n",
      "\n",
      "      10    11  \n",
      "0   down  down  \n",
      "1   down  down  \n",
      "2  right  down  \n",
      "3   None  None  \n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. ¿Cuál es el comportamiento del agente si utilizamos un factor de descuento de 1?\n",
    "\n",
    "En este caso no afecta la politica, ya que independientemente del descuento es mejor tomar el camino más corto, en parte esto se debe a que las acciones son deterministicas, si hubiera ruido en las acciones podríamos esperar cambios en la politica, porque sin el descuento podría valer más la pena evitar estar proximo al acantilado\n",
    "\n",
    "2. ¿Cómo podemos minimizar la trayectoria del agente entre el estado inicial y el estado de llegada?\n",
    "\n",
    "Como dije en el punto anterior al incrementar el descuento se crea un sesgo a menores trayectorias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
